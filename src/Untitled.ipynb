{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "\n",
    "La primera columna \"node\" contiene el nombre del nodo en el que se toman los datos (eg c7102). Para los datos de las enfriadoras, aparecen en esta columna como \"1\" y \"2\" (enfriadora 1, enfriadora 2 respectivamente) para los datos de temperatura y presión y como \"basement\" para los datos de consumo (la enfriadora 1 se corresponde con la columna \"Power13\" y la 2 con \"Power14\").\n",
    "\n",
    "La segunda y tercera columna incluyen los rangos de fecha para las series temporales de esa fila. Como se indicó antes, de 2018/01/01 hasta 2021/01/06.\n",
    "\n",
    "La columna 4 \"power\": incluye los consumos de los nodos (ie node=\"cxxxx\"). En cada celda se encuentra una serie temporal en formato diccionario en el que la key se corresponde con el timestamp del dato.\n",
    "\n",
    "Las columnas 5 y 6 'Power13', 'Power14' se corresponden con el consumo de las enfriadoras (medido en los cuadros PM13 PM14) como se indicó en la explicación de la columna nodo. Solo debería tener datos para node=\"basement\". En cada celda se encuentra una serie temporal en formato diccionario en el que la key se corresponde con el timestamp del dato.\n",
    "\n",
    "Las columnas de la 7 a 10 :'in' -(free cooling)-> 'evaporator' -(compresores)-> 'out'. Y  'ambient' sería la temperatura externa. Estas incluyen las medidas de las temperaturas en las enfriadoras (respectivamente de entrada, salida, en el evaporador y ambiente). Solo debería tener datos para node=\"1\" o \"2\" dependiendo de que enfriadora se trate. En cada celda se encuentra una serie temporal en formato diccionario en el que la key se corresponde con el timestamp del dato.\n",
    "\n",
    "Las columnas 11, 12: 'Compressor1', 'Compressor2' son equivalentes al punto \n",
    "anterior pero con los datos de presión en cada uno de los dos compresores de cada enfriadora. Solo debería tener datos para node=\"1\" o \"2\" dependiendo de que enfriadora se trate. En cada celda se encuentra una serie temporal en formato diccionario en el que la key se corresponde con el timestamp del dato.\n",
    "\n",
    "## Objetivos:\n",
    "Como series temporales usaremos (agrupando cada 30 minutos):\n",
    "### Suma del consumo de los nodos -  Originalmente en Wattios\n",
    "### Suma del consumo de las dos enfriadoras - KW (Power 13 enfriadora 1, Power 14 enfriadora 2)\n",
    "### Máximo de la presión de los 4 compresores - En Pascales (2 enfriadoras, 4 compresores, una lista con dos diccionarios en principio)\n",
    "### Número de compresores activos (compresores con presión mayor a 15 bars) \n",
    "### Cogeremos las temperaturas in, evaporator, out, ambient correspondientes a la enfriadora activa (la que tenga consumo mayor a 10KW) (referencia, agua caliente que volver del CPD es en torno a 18 grados)\n",
    "### Cogeremos la diferencia entre la temperatura ambient y el setpoint (se puede obtener como media de temperatura out) \n",
    "\n",
    "En total tenemos 9 series temporales (a determinar si interesa mantener por separado temperatura ambiente y la diferencia de temperatura entre ambiente y setpoint).\n",
    "\n",
    "Trataremos de hacer:\n",
    "- CU1: Predicción de la suma del consumo de las enfriadoras a 24h (será función de cuanto free cooling se pueda utilizar)\n",
    "- CU2: Predicción de la presión máxima a 24h\n",
    "\n",
    "Para el entrenamiento del CU1 podemos usar datos generales aunque serán más significativos los de invierno (sólo se puede usar free cooling aquellos momentos en que la temperatura ambiente es menor a la temperatura in, en verano esto solo es probable que ocurra durante la noche).\n",
    "\n",
    "Para el entrenamiento del CU2 son sólo relevantes los datos de los meses de verano.\n",
    "\n",
    "\n",
    "Para visualizar los datos se puede usar el siguiente dashboard:\n",
    "\n",
    "Dashboard:\n",
    "\n",
    "http://grafana.srv.cesga.es/d/000000016/dcim?orgId=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cdh61-login8.bd.cluster.cesga.es:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0-cdh6.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6d23502410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Imports\n",
    "import os\n",
    "import copy\n",
    "import statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Spark dependencies\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel, SparkConf\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, PCA\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "#Other configs\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "#Useful directory variables\n",
    "src_path = os.getcwd()\n",
    "root_path = os.path.dirname(src_path)\n",
    "data_path = root_path+\"/datasets\"\n",
    "visualization_path = root_path+\"/data_visualization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer Paso: pasar de una tabla con una unica fila conteniendo las series temporales como diccionarios en una celda (o una lista con dos diccionarios si hay dos series - Compresor 1 y 2 de Enfriadora 1 y lo mismo para enfriadora 2, a series temporales tabulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- node: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- power: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- Power13: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- Power14: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- in: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- out: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- evaporator: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- ambient: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- Compressor1: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- Compressor2: map (nullable = true)\n",
      " |    |-- key: timestamp\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"output_final.parquet\") #Functional programming. Reading the raw data file with the Structured API\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"SELECT node from df\").show(291)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
