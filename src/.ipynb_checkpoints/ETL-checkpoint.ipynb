{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "\n",
    "La primera columna \"node\" contiene el nombre del nodo en el que se toman los datos (eg c7102). Para los datos de las enfriadoras, aparecen en esta columna como \"1\" y \"2\" (enfriadora 1, enfriadora 2 respectivamente) para los datos de temperatura y presión y como \"basement\" para los datos de consumo (la enfriadora 1 se corresponde con la columna \"Power13\" y la 2 con \"Power14\").\n",
    "\n",
    "La segunda y tercera columna incluyen los rangos de fecha para las series temporales de esa fila. Como se indicó antes, de 2018/01/01 hasta 2021/01/06.\n",
    "\n",
    "La columna 4 \"power\": incluye los consumos de los nodos (ie node=\"cxxxx\"). En cada celda se encuentra una serie temporal en formato diccionario en el que la key se corresponde con el timestamp del dato.\n",
    "\n",
    "Las columnas 5 y 6 'Power13', 'Power14' se corresponden con el consumo de las enfriadoras (medido en los cuadros PM13 PM14) como se indicó en la explicación de la columna nodo. Solo debería tener datos para node=\"basement\". En cada celda se encuentra una serie temporal en formato diccionario en el que la key se corresponde con el timestamp del dato.\n",
    "\n",
    "Las columnas de la 7 a 10 :'in' -(free cooling)-> 'evaporator' -(compresores)-> 'out'. Y  'ambient' sería la temperatura externa. Estas incluyen las medidas de las temperaturas en las enfriadoras (respectivamente de entrada, salida, en el evaporador y ambiente). Solo debería tener datos para node=\"1\" o \"2\" dependiendo de que enfriadora se trate. En cada celda se encuentra una serie temporal en formato diccionario en el que la key se corresponde con el timestamp del dato.\n",
    "\n",
    "Las columnas 11, 12: 'Compressor1', 'Compressor2' son equivalentes al punto \n",
    "anterior pero con los datos de presión en cada uno de los dos compresores de cada enfriadora. Solo debería tener datos para node=\"1\" o \"2\" dependiendo de que enfriadora se trate. En cada celda se encuentra una serie temporal en formato diccionario en el que la key se corresponde con el timestamp del dato.\n",
    "\n",
    "## Objetivos:\n",
    "Como series temporales usaremos (agrupando cada 30 minutos):\n",
    "### Suma del consumo de los nodos -  Originalmente en Wattios\n",
    "### Suma del consumo de las dos enfriadoras - KW (Power 13 enfriadora 1, Power 14 enfriadora 2)\n",
    "### Máximo de la presión de los 4 compresores - En Pascales (2 enfriadoras, 4 compresores, una lista con dos diccionarios en principio)\n",
    "### Número de compresores activos (compresores con presión mayor a 15 bars) \n",
    "### Cogeremos las temperaturas in, evaporator, out, ambient correspondientes a la enfriadora activa (la que tenga consumo mayor a 10KW) (referencia, agua caliente que volver del CPD es en torno a 18 grados)\n",
    "### Cogeremos la diferencia entre la temperatura ambient y el setpoint (se puede obtener como media de temperatura out) \n",
    "\n",
    "En total tenemos 9 series temporales (a determinar si interesa mantener por separado temperatura ambiente y la diferencia de temperatura entre ambiente y setpoint).\n",
    "\n",
    "Trataremos de hacer:\n",
    "- CU1: Predicción de la suma del consumo de las enfriadoras a 24h (será función de cuanto free cooling se pueda utilizar)\n",
    "- CU2: Predicción de la presión máxima a 24h\n",
    "\n",
    "Para el entrenamiento del CU1 podemos usar datos generales aunque serán más significativos los de invierno (sólo se puede usar free cooling aquellos momentos en que la temperatura ambiente es menor a la temperatura in, en verano esto solo es probable que ocurra durante la noche).\n",
    "\n",
    "Para el entrenamiento del CU2 son sólo relevantes los datos de los meses de verano.\n",
    "\n",
    "\n",
    "Para visualizar los datos se puede usar el siguiente dashboard:\n",
    "\n",
    "Dashboard:\n",
    "\n",
    "http://grafana.srv.cesga.es/d/000000016/dcim?orgId=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados: \n",
    "Procesado 50 primeras columnas de nodos 1437.9353668689728 segundos = 24 minutos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer Paso: pasar de la tabla con el formato original conteniendo las series temporales como diccionarios en una celda (o una lista con dos diccionarios si hay dos series - Compresor 1 y 2 de Enfriadora 1 y lo mismo para enfriadora 2, a series temporales tabulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Total averga Node electrical consumption/ Watts   obtention\n",
    "## Getting the average node consumption (Watts) averaged in sliding window per 30 minutes since start to end of time sourcing. Then adding all of them in  single time series with the added average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dos columnaas: Time in seconds 86.5683023929596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in seconds 1437.9353668689728\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Spark dependencies\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel, SparkConf\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, PCA\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import col, expr, udf, sequence\n",
    "\n",
    "# Other configs\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# Useful directory variables\n",
    "src_path = os.getcwd()\n",
    "root_path = os.path.dirname(src_path)\n",
    "data_path = root_path + \"/datasets\"\n",
    "visualization_path = root_path + \"/data_visualization\"\n",
    "\n",
    "# Start counting time\n",
    "start_t = time.time()\n",
    "# Reading the original file\n",
    "df = spark.read.parquet(\n",
    "    \"output_final.parquet\"\n",
    ")  # Functional programming. Reading the raw data file with the Structured API\n",
    "# df.printSchema()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# Generating a dataFrame with the times (every 30 minutes from start_timestamp to end_timestamp)\n",
    "dates = pd.date_range(\n",
    "    start=datetime(2018, 1, 1, 0, 0, 0),\n",
    "    end=datetime(2021, 6, 30, 0, 0, 0),\n",
    "    freq=\"30min\",\n",
    ")\n",
    "datetimes = [date.to_pydatetime() for date in dates]\n",
    "time_df = (\n",
    "    spark.createDataFrame(datetimes, TimestampType())\n",
    "    .withColumnRenamed(\"value\", \"time\")\n",
    "    .sort(F.asc(\"time\"))\n",
    ")\n",
    "\n",
    "# Obtaning each consumption node in a list\n",
    "node_list = (\n",
    "    spark.sql(\"SELECT node from df\").rdd.flatMap(lambda x: x).collect()\n",
    ")  # Getting the list with all the node names\n",
    "\n",
    "\n",
    "# Obtaning each consumption node:\n",
    "# We are having two time related Spark Dataframes that originally prior to iterate will be identical\n",
    "# time_df: will remain unchanged during the whole execution, just a reference to ensure all the times are met and if not a null is given for the corresponding electrical consumption column\n",
    "# consumption_df: this will suffer a left join at the end of each iteration and will be the container for all the consumption columns\n",
    "consumption_df = spark.createDataFrame(time_df.rdd, time_df.schema)\n",
    "consumption_df = consumption_df.withColumn(\"total_average_power_consumption_W\", lit(0))\n",
    "for node in node_list[:50]:  # All the consumption related cluster nodes\n",
    "    sql_query_node_consumption = \"\"\"\n",
    "                    SELECT \n",
    "                        EXPLODE(power) as (time, node_{}_power_consumption) \n",
    "                    FROM df\n",
    "                    WHERE \n",
    "                        node LIKE \"{}\"\n",
    "                \"\"\".format(\n",
    "        node, node\n",
    "    )\n",
    "    node_consumption = spark.sql(sql_query_node_consumption)\n",
    "    node_consumption = node_consumption.withColumn(\n",
    "        \"time\", F.to_timestamp(node_consumption.time, \"yyyy-MM-dd HH:MM:SS\")\n",
    "    )\n",
    "    node_consumption = node_consumption.groupBy(\n",
    "        \"time\", F.window(\"time\", \"30 minutes\")\n",
    "    ).agg(\n",
    "        avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "            \"node_{}_power_consumption\".format(node)\n",
    "        ),\n",
    "    )\n",
    "    node_consumption = node_consumption.select(\n",
    "        \"time\", \"window.*\", \"node_{}_power_consumption\".format(node)\n",
    "    ).sort(F.asc(\"time\"))\n",
    "    node_consumption = node_consumption.select(\n",
    "        col(\"end\").alias(\"time\"), col(\"node_{}_power_consumption\".format(node))\n",
    "    )\n",
    "    node_consumption = node_consumption.groupBy(\"time\").agg(\n",
    "        avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "            \"node_{}_average_power_consumption\".format(node)\n",
    "        )\n",
    "    )\n",
    "    node_consumption = node_consumption.select(\n",
    "        \"time\", \"node_{}_average_power_consumption\".format(node)\n",
    "    ).sort(F.asc(\"time\"))\n",
    "    consumption_df = consumption_df.join(node_consumption, [\"time\"], how=\"left\").sort(\n",
    "        F.asc(\"time\")\n",
    "    )\n",
    "    consumption_df = consumption_df.fillna(0, subset=[\"node_{}_average_power_consumption\".format(node)])\n",
    "    consumption_df = consumption_df.withColumn(\"total_average_power_consumption_W\", col(\"total_average_power_consumption_W\")+col(\"node_{}_average_power_consumption\".format(node)))\n",
    "    consumption_df = consumption_df.drop(\"node_{}_average_power_consumption\".format(node))\n",
    "# consumption_df.cache()\n",
    "consumption_df.write.parquet(\"consumption_total_average_30min_W_1_to_50\")\n",
    "end_t = time.time()\n",
    "print(\"Time in seconds \" + str(end_t - start_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time: timestamp, total_average_power_consumption_W: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=spark.read.parquet(\"consumption_total_average_30min_W_1_to_50\").sort(F.asc(\"time\"))\n",
    "a.write.parquet(\"consumption_total_average_30min_W_1_to_50_repartition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:03<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in seconds 2422.4834904670715\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Spark dependencies\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel, SparkConf\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, PCA\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import col, expr, udf, sequence\n",
    "\n",
    "# Other configs\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# Useful directory variables\n",
    "src_path = os.getcwd()\n",
    "root_path = os.path.dirname(src_path)\n",
    "data_path = root_path + \"/datasets\"\n",
    "visualization_path = root_path + \"/data_visualization\"\n",
    "\n",
    "# Start counting time\n",
    "start_t = time.time()\n",
    "# Reading the original file\n",
    "df = spark.read.parquet(\n",
    "    \"output_final.parquet\"\n",
    ")  # Functional programming. Reading the raw data file with the Structured API\n",
    "# df.printSchema()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# Generating a dataFrame with the times (every 30 minutes from start_timestamp to end_timestamp)\n",
    "dates = pd.date_range(\n",
    "    start=datetime(2018, 1, 1, 0, 0, 0),\n",
    "    end=datetime(2021, 6, 30, 0, 0, 0),\n",
    "    freq=\"30min\",\n",
    ")\n",
    "datetimes = [date.to_pydatetime() for date in dates]\n",
    "time_df = (\n",
    "    spark.createDataFrame(datetimes, TimestampType())\n",
    "    .withColumnRenamed(\"value\", \"time\")\n",
    "    .sort(F.asc(\"time\"))\n",
    ")\n",
    "\n",
    "# Obtaning each consumption node in a list\n",
    "node_list = (\n",
    "    spark.sql(\"SELECT node from df\").rdd.flatMap(lambda x: x).collect()\n",
    ")  # Getting the list with all the node names\n",
    "\n",
    "\n",
    "# Obtaning each consumption node:\n",
    "# We are having two time related Spark Dataframes that originally prior to iterate will be identical\n",
    "# time_df: will remain unchanged during the whole execution, just a reference to ensure all the times are met and if not a null is given for the corresponding electrical consumption column\n",
    "# consumption_df: this will suffer a left join at the end of each iteration and will be the container for all the consumption columns\n",
    "consumption_df = spark.createDataFrame(time_df.rdd, time_df.schema)\n",
    "consumption_df = consumption_df.withColumn(\"total_average_power_consumption_W\", lit(0))\n",
    "for node in node_list[50:80]:  # All the consumption related cluster nodes\n",
    "    sql_query_node_consumption = \"\"\"\n",
    "                    SELECT \n",
    "                        EXPLODE(power) as (time, node_{}_power_consumption) \n",
    "                    FROM df\n",
    "                    WHERE \n",
    "                        node LIKE \"{}\"\n",
    "                \"\"\".format(\n",
    "        node, node\n",
    "    )\n",
    "    node_consumption = spark.sql(sql_query_node_consumption)\n",
    "    node_consumption = node_consumption.withColumn(\n",
    "        \"time\", F.to_timestamp(node_consumption.time, \"yyyy-MM-dd HH:MM:SS\")\n",
    "    )\n",
    "    node_consumption = node_consumption.groupBy(\n",
    "        \"time\", F.window(\"time\", \"30 minutes\")\n",
    "    ).agg(\n",
    "        avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "            \"node_{}_power_consumption\".format(node)\n",
    "        ),\n",
    "    )\n",
    "    node_consumption = node_consumption.select(\n",
    "        \"time\", \"window.*\", \"node_{}_power_consumption\".format(node)\n",
    "    ).sort(F.asc(\"time\"))\n",
    "    node_consumption = node_consumption.select(\n",
    "        col(\"end\").alias(\"time\"), col(\"node_{}_power_consumption\".format(node))\n",
    "    )\n",
    "    node_consumption = node_consumption.groupBy(\"time\").agg(\n",
    "        avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "            \"node_{}_average_power_consumption\".format(node)\n",
    "        )\n",
    "    )\n",
    "    node_consumption = node_consumption.select(\n",
    "        \"time\", \"node_{}_average_power_consumption\".format(node)\n",
    "    ).sort(F.asc(\"time\"))\n",
    "    consumption_df = consumption_df.join(node_consumption, [\"time\"], how=\"left\").sort(\n",
    "        F.asc(\"time\")\n",
    "    )\n",
    "    consumption_df = consumption_df.fillna(0, subset=[\"node_{}_average_power_consumption\".format(node)])\n",
    "    consumption_df = consumption_df.withColumn(\"total_average_power_consumption_W\", col(\"total_average_power_consumption_W\")+col(\"node_{}_average_power_consumption\".format(node)))\n",
    "    consumption_df = consumption_df.drop(\"node_{}_average_power_consumption\".format(node))\n",
    "# consumption_df.cache()\n",
    "\n",
    "consumption_df.repartition(1).write.parquet(\"consumption_total_average_30min_W_51_to_80\")\n",
    "end_t = time.time()\n",
    "print(\"Time in seconds \" + str(end_t - start_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos preparando la serie temporal para el consumo electrico de los nodos del cluster en Vatios. Para ello debemos hacer explode de todas las series temporales asociadas a nodos (todas las que estan en una fila cuyo valor en la columna node no sea \"1\", \"2\" o \"basement\". Despues las agruparemos (con media) cada 30 minutos y por ultima realizaremos la suma para tener el total de consumo medio en Vatios cada 30 minutos. Es importante notar que no se puede sumar primero y luego agrupar ya que cada una de las series temporales tiene medidas para un valor de tiempo distinto (difieren en el orden de los segundos) tal y como se ve para c6601 y c7102 en el notebook eDA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IDEA ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_consumption_pandas=node_consumption.limit(1000).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>electric_consumption_node_c6601</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:44</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:01:57</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:03:09</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:04:21</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 00:05:33</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-01 00:06:45</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-01 00:07:57</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-01 00:09:08</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-01 00:10:20</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-01 00:11:33</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-01 00:12:45</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-01 00:13:57</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-01 00:15:09</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-01 00:16:20</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-01 00:17:32</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-01 00:18:44</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-01 00:19:57</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-01 00:21:09</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-01 00:22:20</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-01 00:23:31</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-01 00:24:43</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-01 00:25:55</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-01-01 00:27:07</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-01 00:28:19</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-01-01 00:29:31</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-01 00:30:43</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-01-01 00:31:59</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-01-01 00:33:23</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-01 00:34:46</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-01-01 00:36:12</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  electric_consumption_node_c6601\n",
       "0  2018-01-01 00:00:44                            54.00\n",
       "1  2018-01-01 00:01:57                            54.00\n",
       "2  2018-01-01 00:03:09                            72.00\n",
       "3  2018-01-01 00:04:21                            54.00\n",
       "4  2018-01-01 00:05:33                            54.00\n",
       "5  2018-01-01 00:06:45                            54.00\n",
       "6  2018-01-01 00:07:57                            54.00\n",
       "7  2018-01-01 00:09:08                            54.00\n",
       "8  2018-01-01 00:10:20                            54.00\n",
       "9  2018-01-01 00:11:33                            90.00\n",
       "10 2018-01-01 00:12:45                            54.00\n",
       "11 2018-01-01 00:13:57                            54.00\n",
       "12 2018-01-01 00:15:09                            54.00\n",
       "13 2018-01-01 00:16:20                            54.00\n",
       "14 2018-01-01 00:17:32                            54.00\n",
       "15 2018-01-01 00:18:44                            54.00\n",
       "16 2018-01-01 00:19:57                            54.00\n",
       "17 2018-01-01 00:21:09                            54.00\n",
       "18 2018-01-01 00:22:20                            54.00\n",
       "19 2018-01-01 00:23:31                            72.00\n",
       "20 2018-01-01 00:24:43                            54.00\n",
       "21 2018-01-01 00:25:55                            54.00\n",
       "22 2018-01-01 00:27:07                            54.00\n",
       "23 2018-01-01 00:28:19                            54.00\n",
       "24 2018-01-01 00:29:31                            54.00\n",
       "25 2018-01-01 00:30:43                            54.00\n",
       "26 2018-01-01 00:31:59                            54.00\n",
       "27 2018-01-01 00:33:23                            54.00\n",
       "28 2018-01-01 00:34:46                            54.00\n",
       "29 2018-01-01 00:36:12                            54.00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_consumption_pandas.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>window</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 12:33:04</td>\n",
       "      <td>(2018-01-01 12:30:00, 2018-01-01 13:00:00)</td>\n",
       "      <td>108.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 16:33:32</td>\n",
       "      <td>(2018-01-01 16:30:00, 2018-01-01 17:00:00)</td>\n",
       "      <td>108.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 19:55:46</td>\n",
       "      <td>(2018-01-01 19:30:00, 2018-01-01 20:00:00)</td>\n",
       "      <td>108.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02 02:18:10</td>\n",
       "      <td>(2018-01-02 02:00:00, 2018-01-02 02:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02 06:36:48</td>\n",
       "      <td>(2018-01-02 06:30:00, 2018-01-02 07:00:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-02 06:53:33</td>\n",
       "      <td>(2018-01-02 06:30:00, 2018-01-02 07:00:00)</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-02 07:19:53</td>\n",
       "      <td>(2018-01-02 07:00:00, 2018-01-02 07:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-02 09:28:53</td>\n",
       "      <td>(2018-01-02 09:00:00, 2018-01-02 09:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-02 13:27:24</td>\n",
       "      <td>(2018-01-02 13:00:00, 2018-01-02 13:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-03 00:18:36</td>\n",
       "      <td>(2018-01-03 00:00:00, 2018-01-03 00:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-03 06:16:58</td>\n",
       "      <td>(2018-01-03 06:00:00, 2018-01-03 06:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-03 16:32:15</td>\n",
       "      <td>(2018-01-03 16:30:00, 2018-01-03 17:00:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-04 21:22:35</td>\n",
       "      <td>(2018-01-04 21:00:00, 2018-01-04 21:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-05 13:32:04</td>\n",
       "      <td>(2018-01-05 13:30:00, 2018-01-05 14:00:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-05 22:09:46</td>\n",
       "      <td>(2018-01-05 22:00:00, 2018-01-05 22:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-06 02:10:07</td>\n",
       "      <td>(2018-01-06 02:00:00, 2018-01-06 02:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-06 10:28:59</td>\n",
       "      <td>(2018-01-06 10:00:00, 2018-01-06 10:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-06 11:56:12</td>\n",
       "      <td>(2018-01-06 11:30:00, 2018-01-06 12:00:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-06 13:06:52</td>\n",
       "      <td>(2018-01-06 13:00:00, 2018-01-06 13:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-06 19:29:10</td>\n",
       "      <td>(2018-01-06 19:00:00, 2018-01-06 19:30:00)</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time                                      window  power\n",
       "0  2018-01-01 12:33:04  (2018-01-01 12:30:00, 2018-01-01 13:00:00) 108.00\n",
       "1  2018-01-01 16:33:32  (2018-01-01 16:30:00, 2018-01-01 17:00:00) 108.00\n",
       "2  2018-01-01 19:55:46  (2018-01-01 19:30:00, 2018-01-01 20:00:00) 108.00\n",
       "3  2018-01-02 02:18:10  (2018-01-02 02:00:00, 2018-01-02 02:30:00)  54.00\n",
       "4  2018-01-02 06:36:48  (2018-01-02 06:30:00, 2018-01-02 07:00:00)  54.00\n",
       "5  2018-01-02 06:53:33  (2018-01-02 06:30:00, 2018-01-02 07:00:00)  90.00\n",
       "6  2018-01-02 07:19:53  (2018-01-02 07:00:00, 2018-01-02 07:30:00)  54.00\n",
       "7  2018-01-02 09:28:53  (2018-01-02 09:00:00, 2018-01-02 09:30:00)  54.00\n",
       "8  2018-01-02 13:27:24  (2018-01-02 13:00:00, 2018-01-02 13:30:00)  54.00\n",
       "9  2018-01-03 00:18:36  (2018-01-03 00:00:00, 2018-01-03 00:30:00)  54.00\n",
       "10 2018-01-03 06:16:58  (2018-01-03 06:00:00, 2018-01-03 06:30:00)  54.00\n",
       "11 2018-01-03 16:32:15  (2018-01-03 16:30:00, 2018-01-03 17:00:00)  54.00\n",
       "12 2018-01-04 21:22:35  (2018-01-04 21:00:00, 2018-01-04 21:30:00)  54.00\n",
       "13 2018-01-05 13:32:04  (2018-01-05 13:30:00, 2018-01-05 14:00:00)  54.00\n",
       "14 2018-01-05 22:09:46  (2018-01-05 22:00:00, 2018-01-05 22:30:00)  54.00\n",
       "15 2018-01-06 02:10:07  (2018-01-06 02:00:00, 2018-01-06 02:30:00)  54.00\n",
       "16 2018-01-06 10:28:59  (2018-01-06 10:00:00, 2018-01-06 10:30:00)  54.00\n",
       "17 2018-01-06 11:56:12  (2018-01-06 11:30:00, 2018-01-06 12:00:00)  54.00\n",
       "18 2018-01-06 13:06:52  (2018-01-06 13:00:00, 2018-01-06 13:30:00)  54.00\n",
       "19 2018-01-06 19:29:10  (2018-01-06 19:00:00, 2018-01-06 19:30:00)  54.00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "fecha_fija = \"2018-01-01 00:00:00\"\n",
    "start_time = datetime.strptime(fecha_fija,\"%Y-%m-%d %H:%M:%S\")\n",
    "minutes_since_1970_to_start_time = int(time.mktime(start_time.timetuple())/60)#Passing the total seconds to minutes dividing by 60\n",
    "offset_minutes = minutes_since_1970_to_start_time % 30 #offset minutes\n",
    "window_30_mins = F.window(\"time\", \"30 minutes\", startTime = \"{} minutes\".format(offset_minutes))\n",
    "pandas=node_consumption.groupBy(\"time\", window_30_mins).agg(F.mean(\"electric_consumption_node_c6601\").alias(\"power\")).limit(20).toPandas()\n",
    "pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fecha_fija = \"2018-01-01 00:00:00\"\n",
    "start_time = datetime.strptime(fecha_fija,\"%Y-%m-%d %H:%M:%S\")\n",
    "start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BORRADOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|         start_time|           end_time|\n",
      "+-------------------+-------------------+\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "|2018-01-01 01:00:00|2021-06-01 02:00:00|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates.show()#All the nodes has the same starting and ending date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Spark dependencies\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel, SparkConf\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, PCA\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import TimestampType \n",
    "from pyspark.sql.functions import col, expr,udf, sequence\n",
    "\n",
    "# Other configs\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# Useful directory variables\n",
    "src_path = os.getcwd()\n",
    "root_path = os.path.dirname(src_path)\n",
    "data_path = root_path + \"/datasets\"\n",
    "visualization_path = root_path + \"/data_visualization\"\n",
    "\n",
    "#Start counting time\n",
    "start_t = time.time()\n",
    "#Reading the original file\n",
    "df = spark.read.parquet(\n",
    "    \"output_final.parquet\"\n",
    ")  # Functional programming. Reading the raw data file with the Structured API\n",
    "# df.printSchema()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "#Generating a dataFrame with the times (every 30 minutes from start_timestamp to end_timestamp)\n",
    "dates = pd.date_range(start=datetime(2018, 1, 1, 0,0,0), end=datetime(2021, 6, 30, 0,0,0), freq=\"30min\")\n",
    "datetimes = [date.to_pydatetime() for date in dates]\n",
    "time_df = spark.createDataFrame(datetimes, TimestampType()).withColumnRenamed(\"value\", \"time\").sort(F.asc(\"time\"))\n",
    "\n",
    "#Obtaning each consumption node\n",
    "node_list = (\n",
    "    spark.sql(\"SELECT node from df\").rdd.flatMap(lambda x: x).collect()\n",
    ")  # Getting the list with all the node names\n",
    "# sql_query_dates = \"\"\"\n",
    "#                 SELECT \n",
    "#                     start_time,\n",
    "#                     end_time\n",
    "#                 FROM df\n",
    "\n",
    "#             \"\"\".format(\n",
    "#     node_list[0], node_list[0]\n",
    "# )\n",
    "sql_query_node_consumption = \"\"\"\n",
    "                SELECT \n",
    "                    EXPLODE(power) as (time, electric_consumption_node_{}) \n",
    "                FROM df\n",
    "                WHERE \n",
    "                    node LIKE \"{}\"\n",
    "            \"\"\".format(\n",
    "    node_list[0], node_list[0]\n",
    ")\n",
    "node_consumption = spark.sql(sql_query_node_consumption)\n",
    "# dates = spark.sql(sql_query_dates)\n",
    "node_consumption = node_consumption.withColumn(\n",
    "    \"time\", F.to_timestamp(node_consumption.time, \"yyyy-MM-dd HH:MM:SS\")\n",
    ")\n",
    "node_consumption = node_consumption.groupBy(\"time\", F.window(\"time\", \"30 minutes\")).agg(\n",
    "    avg(\"electric_consumption_node_c6601\").alias(\"electric_consumption_node_c6601\"),\n",
    ")\n",
    "node_consumption = node_consumption.select(\n",
    "    \"time\", \"window.*\", \"electric_consumption_node_c6601\"\n",
    ").sort(F.asc(\"time\"))\n",
    "node_consumption = node_consumption.select(\n",
    "    col(\"end\").alias(\"time\"), col(\"electric_consumption_node_c6601\")\n",
    ")\n",
    "node_consumption = node_consumption.groupBy(\"time\").agg(\n",
    "    avg(\"electric_consumption_node_c6601\").alias(\n",
    "        \"electric_consumption_node_c6601_average\"\n",
    "    )\n",
    ")\n",
    "node_consumption = node_consumption.select(\n",
    "    \"time\", \"electric_consumption_node_c6601_average\"\n",
    ").sort(F.asc(\"time\"))\n",
    "node_consumption.show(40)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_t=time.time()\n",
    "print(\"Time in seconds \"+str(end_t-start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaning each consumption node\n",
    "#for node in node_list[:-3]:\n",
    "#We are having two time related Spark Dataframes that originally prior to iterate will be identical\n",
    "#time_df: will remain unchanged during the whole execution, just a reference to ensure all the times are met and if not a null is given for the corresponding electrical consumption column\n",
    "#consumption_df: this will suffer a left join at the end of each iteration and will be the container for all the consumption columns\n",
    "consumption_df = spark.createDataFrame(time_df.rdd, time_df.schema)\n",
    "for node in node_list[:3]:\n",
    "    sql_query_node_consumption = \"\"\"\n",
    "                    SELECT \n",
    "                        EXPLODE(power) as (time, node_{}_power_consumption) \n",
    "                    FROM df\n",
    "                    WHERE \n",
    "                        node LIKE \"{}\"\n",
    "                \"\"\".format(\n",
    "        node, node\n",
    "    )\n",
    "    node_consumption = spark.sql(sql_query_node_consumption)\n",
    "    node_consumption = node_consumption.withColumn(\n",
    "        \"time\", F.to_timestamp(node_consumption.time, \"yyyy-MM-dd HH:MM:SS\")\n",
    "    )\n",
    "    node_consumption = node_consumption.groupBy(\"time\", F.window(\"time\", \"30 minutes\")).agg(\n",
    "        avg(\"node_{}_power_consumption\".format(node)).alias(\"node_{}_power_consumption\".format(node)),\n",
    "    )\n",
    "    node_consumption = node_consumption.select(\n",
    "        \"time\", \"window.*\", \"node_{}_power_consumption\".format(node)\n",
    "    ).sort(F.asc(\"time\"))\n",
    "    node_consumption = node_consumption.select(\n",
    "        col(\"end\").alias(\"time\"), col(\"node_{}_power_consumption\".format(node))\n",
    "    )\n",
    "    node_consumption = node_consumption.groupBy(\"time\").agg(\n",
    "        avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "            \"node_{}_average_power_consumption\".format(node)\n",
    "        )\n",
    "    )\n",
    "    node_consumption = node_consumption.select(\n",
    "        \"time\", \"node_{}_average_power_consumption\".format(node)\n",
    "    ).sort(F.asc(\"time\"))\n",
    "    consumption_df = consumption_df.join(node_consumption, [\"time\"], how=\"left\").sort(F.asc(\"time\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  For cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Spark dependencies\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel, SparkConf\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, PCA\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import col, expr, udf, sequence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Generate_20_columns') \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    # Other configs\n",
    "    pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "    # Useful directory variables\n",
    "    src_path = os.getcwd()\n",
    "    root_path = os.path.dirname(src_path)\n",
    "    data_path = root_path + \"/datasets\"\n",
    "    visualization_path = root_path + \"/data_visualization\"\n",
    "\n",
    "    # Start counting time\n",
    "    start_t = time.time()\n",
    "    # Reading the original file\n",
    "    df = spark.read.parquet(\n",
    "        \"output_final.parquet\"\n",
    "    )  # Functional programming. Reading the raw data file with the Structured API\n",
    "    # df.printSchema()\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "    # Generating a dataFrame with the times (every 30 minutes from start_timestamp to end_timestamp)\n",
    "    dates = pd.date_range(\n",
    "        start=datetime(2018, 1, 1, 0, 0, 0),\n",
    "        end=datetime(2021, 6, 30, 0, 0, 0),\n",
    "        freq=\"30min\",\n",
    "    )\n",
    "    datetimes = [date.to_pydatetime() for date in dates]\n",
    "    time_df = (\n",
    "        spark.createDataFrame(datetimes, TimestampType())\n",
    "        .withColumnRenamed(\"value\", \"time\")\n",
    "        .sort(F.asc(\"time\"))\n",
    "    )\n",
    "\n",
    "    # Obtaning each consumption node in a list\n",
    "    node_list = (\n",
    "        spark.sql(\"SELECT node from df\").rdd.flatMap(lambda x: x).collect()\n",
    "    )  # Getting the list with all the node names\n",
    "\n",
    "\n",
    "    # Obtaning each consumption node:\n",
    "    # We are having two time related Spark Dataframes that originally prior to iterate will be identical\n",
    "    # time_df: will remain unchanged during the whole execution, just a reference to ensure all the times are met and if not a null is given for the corresponding electrical consumption column\n",
    "    # consumption_df: this will suffer a left join at the end of each iteration and will be the container for all the consumption columns\n",
    "    consumption_df = spark.createDataFrame(time_df.rdd, time_df.schema)\n",
    "    for node in node_list[:20]:  # All the consumption related cluster nodes\n",
    "        sql_query_node_consumption = \"\"\"\n",
    "                        SELECT \n",
    "                            EXPLODE(power) as (time, node_{}_power_consumption) \n",
    "                        FROM df\n",
    "                        WHERE \n",
    "                            node LIKE \"{}\"\n",
    "                    \"\"\".format(\n",
    "            node, node\n",
    "        )\n",
    "        node_consumption = spark.sql(sql_query_node_consumption)\n",
    "        node_consumption = node_consumption.withColumn(\n",
    "            \"time\", F.to_timestamp(node_consumption.time, \"yyyy-MM-dd HH:MM:SS\")\n",
    "        )\n",
    "        node_consumption = node_consumption.groupBy(\n",
    "            \"time\", F.window(\"time\", \"30 minutes\")\n",
    "        ).agg(\n",
    "            avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "                \"node_{}_power_consumption\".format(node)\n",
    "            ),\n",
    "        )\n",
    "        node_consumption = node_consumption.select(\n",
    "            \"time\", \"window.*\", \"node_{}_power_consumption\".format(node)\n",
    "        ).sort(F.asc(\"time\"))\n",
    "        node_consumption = node_consumption.select(\n",
    "            col(\"end\").alias(\"time\"), col(\"node_{}_power_consumption\".format(node))\n",
    "        )\n",
    "        node_consumption = node_consumption.groupBy(\"time\").agg(\n",
    "            avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "                \"node_{}_average_power_consumption\".format(node)\n",
    "            )\n",
    "        )\n",
    "        node_consumption = node_consumption.select(\n",
    "            \"time\", \"node_{}_average_power_consumption\".format(node)\n",
    "        ).sort(F.asc(\"time\"))\n",
    "        consumption_df = consumption_df.j oin(node_consumption, [\"time\"], how=\"left\").sort(\n",
    "            F.asc(\"time\")\n",
    "        )\n",
    "    consumption_df.cache()\n",
    "    consumption_df.write.parquet(\"consumption_per_cluster_node_1to20_average_30min\")\n",
    "    end_t = time.time()\n",
    "    print(\"Time in seconds \" + str(end_t - start_t))\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Spark dependencies\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel, SparkConf\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, PCA\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import col, expr, udf, sequence\n",
    "\n",
    "# Other configs\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# Useful directory variables\n",
    "src_path = os.getcwd()\n",
    "root_path = os.path.dirname(src_path)\n",
    "data_path = root_path + \"/datasets\"\n",
    "visualization_path = root_path + \"/data_visualization\"\n",
    "\n",
    "# Start counting time\n",
    "start_t = time.time()\n",
    "# Reading the original file\n",
    "df = spark.read.parquet(\n",
    "    \"output_final.parquet\"\n",
    ")  # Functional programming. Reading the raw data file with the Structured API\n",
    "# df.printSchema()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# Generating a dataFrame with the times (every 30 minutes from start_timestamp to end_timestamp)\n",
    "dates = pd.date_range(\n",
    "    start=datetime(2018, 1, 1, 0, 0, 0),\n",
    "    end=datetime(2021, 6, 30, 0, 0, 0),\n",
    "    freq=\"30min\",\n",
    ")\n",
    "datetimes = [date.to_pydatetime() for date in dates]\n",
    "time_df = (\n",
    "    spark.createDataFrame(datetimes, TimestampType())\n",
    "    .withColumnRenamed(\"value\", \"time\")\n",
    "    .sort(F.asc(\"time\"))\n",
    ")\n",
    "\n",
    "# Obtaning each consumption node in a list\n",
    "node_list = (\n",
    "    spark.sql(\"SELECT node from df\").rdd.flatMap(lambda x: x).collect()\n",
    ")  # Getting the list with all the node names\n",
    "\n",
    "\n",
    "# Obtaning each consumption node:\n",
    "# We are having two time related Spark Dataframes that originally prior to iterate will be identical\n",
    "# time_df: will remain unchanged during the whole execution, just a reference to ensure all the times are met and if not a null is given for the corresponding electrical consumption column\n",
    "# consumption_df: this will suffer a left join at the end of each iteration and will be the container for all the consumption columns\n",
    "consumption_df = spark.createDataFrame(time_df.rdd, time_df.schema)\n",
    "for node in node_list[:30]:  # All the consumption related cluster nodes\n",
    "    sql_query_node_consumption = \"\"\"\n",
    "                    SELECT \n",
    "                        EXPLODE(power) as (time, node_{}_power_consumption) \n",
    "                    FROM df\n",
    "                    WHERE \n",
    "                        node LIKE \"{}\"\n",
    "                \"\"\".format(\n",
    "        node, node\n",
    "    )\n",
    "    node_consumption = spark.sql(sql_query_node_consumption)\n",
    "    node_consumption = node_consumption.withColumn(\n",
    "        \"time\", F.to_timestamp(node_consumption.time, \"yyyy-MM-dd HH:MM:SS\")\n",
    "    )\n",
    "    node_consumption = node_consumption.groupBy(\n",
    "        \"time\", F.window(\"time\", \"30 minutes\")\n",
    "    ).agg(\n",
    "        avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "            \"node_{}_power_consumption\".format(node)\n",
    "        ),\n",
    "    )\n",
    "    node_consumption = node_consumption.select(\n",
    "        \"time\", \"window.*\", \"node_{}_power_consumption\".format(node)\n",
    "    ).sort(F.asc(\"time\"))\n",
    "    node_consumption = node_consumption.select(\n",
    "        col(\"end\").alias(\"time\"), col(\"node_{}_power_consumption\".format(node))\n",
    "    )\n",
    "    node_consumption = node_consumption.groupBy(\"time\").agg(\n",
    "        avg(\"node_{}_power_consumption\".format(node)).alias(\n",
    "            \"node_{}_average_power_consumption\".format(node)\n",
    "        )\n",
    "    )\n",
    "    node_consumption = node_consumption.select(\n",
    "        \"time\", \"node_{}_average_power_consumption\".format(node)\n",
    "    ).sort(F.asc(\"time\"))\n",
    "    consumption_df = consumption_df.join(node_consumption, [\"time\"], how=\"left\").sort(\n",
    "        F.asc(\"time\")\n",
    "    )\n",
    "consumption_df.cache()\n",
    "consumption_df.write.parquet(\"consumption_per_cluster_node_average_30min_final\")\n",
    "end_t = time.time()\n",
    "print(\"Time in seconds \" + str(end_t - start_t))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
